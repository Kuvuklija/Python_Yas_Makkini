{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0fa7169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/17 11:26:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+-----------+--------------------+\n",
      "| id|authorId|               title|releaseDate|                link|\n",
      "+---+--------+--------------------+-----------+--------------------+\n",
      "|  1|       1|Fantastic Beasts ...|   11/18/16|http://amzn.to/2k...|\n",
      "|  2|       1|Harry Potter and ...|    10/6/15|http://amzn.to/2l...|\n",
      "|  3|       1|The Tales of Beed...|    12/4/08|http://amzn.to/2k...|\n",
      "|  4|       1|Harry Potter and ...|    10/4/16|http://amzn.to/2k...|\n",
      "|  5|       2|Informix 12.10 on...|    4/23/17|http://amzn.to/2i...|\n",
      "+---+--------+--------------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "__file__ = '/home/vk/Spark_Source_Code/net.jgp.books.spark.ch01/data'\n",
    "current_dir = os.path.dirname(__file__)\n",
    "relative_path = __file__ + \"/books.csv\"\n",
    "absolute_file_path = os.path.join(current_dir, relative_path)\n",
    "\n",
    "# Creates a session on a local master\n",
    "session = SparkSession.builder.appName(\"CSV to Dataset\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# Reads a CSV file with header, called books.csv, stores it in a dataframe\n",
    "df = session.read.csv(header=True, inferSchema=True, path=absolute_file_path)\n",
    "\n",
    "# Shows at most 5 rows from the dataframe\n",
    "df.show(5)\n",
    "\n",
    "# Good to stop SparkSession at the end of the application\n",
    "session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a84b6c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/17 19:35:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o41.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:766)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m prop \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.postgresql.Driver\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvadim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Write in a table called ch02\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdbConnectionUrl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mch02\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Good to stop SparkSession at the end of the application\u001b[39;00m\n\u001b[1;32m     45\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/readwriter.py:1984\u001b[0m, in \u001b[0;36mDataFrameWriter.jdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   1982\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[1;32m   1983\u001b[0m     jprop\u001b[38;5;241m.\u001b[39msetProperty(k, properties[k])\n\u001b[0;32m-> 1984\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o41.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:766)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/17 19:35:51 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "  CSV to a relational database.\n",
    "\n",
    "  @author rambabu.posa\n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "\n",
    "__file__ = '/home/vk/Spark_Source_Code/net.jgp.books.spark.ch02/data'\n",
    "current_dir = os.path.dirname(__file__)\n",
    "relative_path = __file__ + \"/authors.csv\"\n",
    "absolute_file_path = os.path.join(current_dir, relative_path)\n",
    "\n",
    "# Creates a session on a local master\n",
    "spark = SparkSession.builder.appName(\"CSV to DB\").master(\"local\").getOrCreate()\n",
    "\n",
    "#  Step 1: Ingestion\n",
    "#  ---------\n",
    "#\n",
    "#  Reads a CSV file with header, called authors.csv, stores it in a dataframe\n",
    "df = spark.read.csv(header=True, inferSchema=True, path=absolute_file_path)\n",
    "\n",
    "# Step 2: Transform\n",
    "# ---------\n",
    "# Creates a new column called \"name\" as the concatenation of lname, a\n",
    "# virtual column containing \", \" and the fname column\n",
    "df = df.withColumn(\"name\", F.concat(F.col(\"lname\"), F.lit(\", \"), F.col(\"fname\")))\n",
    "\n",
    "# Step 3: Save\n",
    "# ----\n",
    "#\n",
    "# The connection URL, assuming your PostgreSQL instance runs locally on the\n",
    "# default port, and the database we use is \"spark_labs\"\n",
    "dbConnectionUrl = \"jdbc:postgresql://localhost:5432/spark_labs\"\n",
    "\n",
    "# Properties to connect to the database, the JDBC driver is part of our pom.xml\n",
    "prop = {\"driver\":\"org.postgresql.Driver\", \"user\":\"vadim\", \"password\":\"1\"}\n",
    "\n",
    "# Write in a table called ch02\n",
    "df.write.jdbc(mode='overwrite', url=dbConnectionUrl, table=\"ch02\", properties=prop)\n",
    "\n",
    "# Good to stop SparkSession at the end of the application\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c32a1aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Type of df is <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "WARNING:root:*** Right after ingestion\n",
      "WARNING:root:We have 3440 records.\n",
      "WARNING:root:*** Dataframe transformed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+--------------------+--------+-----------+-----+----------+--------------+-------------------+-----------------+--------+------------+-----------+-------------+\n",
      "|OBJECTID|    HSISID|                NAME|            ADDRESS1|ADDRESS2|       CITY|STATE|POSTALCODE|   PHONENUMBER| RESTAURANTOPENDATE|     FACILITYTYPE|PERMITID|           X|          Y|GEOCODESTATUS|\n",
      "+--------+----------+--------------------+--------------------+--------+-----------+-----+----------+--------------+-------------------+-----------------+--------+------------+-----------+-------------+\n",
      "|    1001|4092016024|                WABA|2502 1/2 HILLSBOR...|    NULL|    RALEIGH|   NC|     27607|(919) 833-1710|2011-10-18 04:00:00|       Restaurant|    6952|-78.66818477|35.78783803|            M|\n",
      "|    1002|4092021693|  WALMART DELI #2247|2010 KILDAIRE FAR...|    NULL|       CARY|   NC|     27518|(919) 852-6651|2011-11-08 04:00:00|       Food Stand|    6953|-78.78211173|35.73717591|            M|\n",
      "|    1003|4092017012|CAROLINA SUSHI &a...|5951-107 POYNER V...|    NULL|    RALEIGH|   NC|     27616|(919) 981-5835|2015-08-28 03:00:00|       Restaurant|    6961|-78.57030208|35.86511564|            M|\n",
      "|    1004|4092030288|THE CORNER VENEZU...|    7500 RAMBLE WAY |    NULL|    RALEIGH|   NC|     27616|          NULL|2015-09-04 03:00:00|Mobile Food Units|    6962|  -78.537511|35.87630712|            M|\n",
      "|    1005|4092015530|        SUBWAY #3726| 12233 CAPITAL BLVD |    NULL|WAKE FOREST|   NC|27587-6200|(919) 556-8266|2009-12-11 03:00:00|       Restaurant|    6972|-78.54097555|35.98087357|            M|\n",
      "+--------+----------+--------------------+--------------------+--------+-----------+-----+----------+--------------+-------------------+-----------------+--------+------------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- OBJECTID: integer (nullable = true)\n",
      " |-- HSISID: long (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- ADDRESS1: string (nullable = true)\n",
      " |-- ADDRESS2: string (nullable = true)\n",
      " |-- CITY: string (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- POSTALCODE: string (nullable = true)\n",
      " |-- PHONENUMBER: string (nullable = true)\n",
      " |-- RESTAURANTOPENDATE: timestamp (nullable = true)\n",
      " |-- FACILITYTYPE: string (nullable = true)\n",
      " |-- PERMITID: integer (nullable = true)\n",
      " |-- X: double (nullable = true)\n",
      " |-- Y: double (nullable = true)\n",
      " |-- GEOCODESTATUS: string (nullable = true)\n",
      "\n",
      "+----------+--------------------+--------------------+--------+-----------+-----+----------+--------------+-------------------+-----------------+------------+-----------+------+------------------+\n",
      "| datasetId|                name|            address1|address2|       city|state|       zip|           tel|          dateStart|             type|        geoX|       geoY|county|                id|\n",
      "+----------+--------------------+--------------------+--------+-----------+-----+----------+--------------+-------------------+-----------------+------------+-----------+------+------------------+\n",
      "|4092016024|                WABA|2502 1/2 HILLSBOR...|    NULL|    RALEIGH|   NC|     27607|(919) 833-1710|2011-10-18 04:00:00|       Restaurant|-78.66818477|35.78783803|  Wake|NC_Wake_4092016024|\n",
      "|4092021693|  WALMART DELI #2247|2010 KILDAIRE FAR...|    NULL|       CARY|   NC|     27518|(919) 852-6651|2011-11-08 04:00:00|       Food Stand|-78.78211173|35.73717591|  Wake|NC_Wake_4092021693|\n",
      "|4092017012|CAROLINA SUSHI &a...|5951-107 POYNER V...|    NULL|    RALEIGH|   NC|     27616|(919) 981-5835|2015-08-28 03:00:00|       Restaurant|-78.57030208|35.86511564|  Wake|NC_Wake_4092017012|\n",
      "|4092030288|THE CORNER VENEZU...|    7500 RAMBLE WAY |    NULL|    RALEIGH|   NC|     27616|          NULL|2015-09-04 03:00:00|Mobile Food Units|  -78.537511|35.87630712|  Wake|NC_Wake_4092030288|\n",
      "|4092015530|        SUBWAY #3726| 12233 CAPITAL BLVD |    NULL|WAKE FOREST|   NC|27587-6200|(919) 556-8266|2009-12-11 03:00:00|       Restaurant|-78.54097555|35.98087357|  Wake|NC_Wake_4092015530|\n",
      "+----------+--------------------+--------------------+--------+-----------+-----+----------+--------------+-------------------+-----------------+------------+-----------+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:=============================>                             (2 + 2) / 4]\r",
      "\r",
      "                                                                                \r",
      "WARNING:root:RDD : 3440\n"
     ]
    }
   ],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@@@ ch03 @@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "\"\"\"\n",
    "  CSV ingestion in a dataframe and manipulation.\n",
    "\n",
    "  @author rambabu.posa\n",
    "\"\"\"\n",
    "import os\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "#from pyspark.sql.functions import F\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def get_absolute_file_path(path, filename):\n",
    "    # To get absolute path for a given filename\n",
    "    file = '/home/vk/Spark_Source_Code/net.jgp.books.spark.ch03/data/'\n",
    "    current_dir = os.path.dirname(file)\n",
    "    relative_path = \"{}{}\".format(path, filename)\n",
    "    absolute_file_path = os.path.join(current_dir, relative_path)\n",
    "    return absolute_file_path\n",
    "\n",
    "def main(spark):\n",
    "    # The processing code.\n",
    "    filename = 'Restaurants_in_Wake_County_NC.csv'\n",
    "    path = \"\"\n",
    "    absolute_file_path = get_absolute_file_path(path, filename)\n",
    "\n",
    "    # Reads a CSV file with header, called\n",
    "    # Restaurants_in_Wake_County_NC.csv,\n",
    "    # stores it in a dataframe\n",
    "    # Creates a session on a local master\n",
    "  #  spark = SparkSession.builder.appName(\"CSV to DB\").master(\"local\").getOrCreate()\n",
    "    df = spark.read.csv(header=True, inferSchema=True,path=absolute_file_path)\n",
    "    \n",
    "    logging.warning(\"Type of df is {}\".format(type(df)))\n",
    "    logging.warning(\"*** Right after ingestion\")\n",
    "    \n",
    "    df.show(5)\n",
    "    df.printSchema()\n",
    "    \n",
    "    logging.warning(\"We have {} records.\".format(df.count()))\n",
    "\n",
    "    # Let's transform our dataframe\n",
    "    df =  df.withColumn(\"county\", F.lit(\"Wake\")) \\\n",
    "            .withColumnRenamed(\"HSISID\", \"datasetId\") \\\n",
    "            .withColumnRenamed(\"NAME\", \"name\") \\\n",
    "            .withColumnRenamed(\"ADDRESS1\", \"address1\") \\\n",
    "            .withColumnRenamed(\"ADDRESS2\", \"address2\") \\\n",
    "            .withColumnRenamed(\"CITY\", \"city\") \\\n",
    "            .withColumnRenamed(\"STATE\", \"state\") \\\n",
    "            .withColumnRenamed(\"POSTALCODE\", \"zip\") \\\n",
    "            .withColumnRenamed(\"PHONENUMBER\", \"tel\") \\\n",
    "            .withColumnRenamed(\"RESTAURANTOPENDATE\", \"dateStart\") \\\n",
    "            .withColumnRenamed(\"FACILITYTYPE\", \"type\") \\\n",
    "            .withColumnRenamed(\"X\", \"geoX\") \\\n",
    "            .withColumnRenamed(\"Y\", \"geoY\") \\\n",
    "            .drop(\"OBJECTID\", \"PERMITID\", \"GEOCODESTATUS\")\n",
    "\n",
    "    df = df.withColumn(\"id\",\n",
    "                       F.concat(F.col(\"state\"), F.lit(\"_\"),\n",
    "                                F.col(\"county\"), F.lit(\"_\"),\n",
    "                                F.col(\"datasetId\")))\n",
    "\n",
    "    # Shows at most 5 rows from the dataframe\n",
    "    logging.warning(\"*** Dataframe transformed\")\n",
    "    df.show(5)\n",
    "    \n",
    "    df = df.repartition(4)\n",
    "    # logging.warning(\"RDD : {}\".format(df.rdd.count()))  ---> 3440 records\n",
    "    # print(\"json : {}\".format(df.toJSON().collect())) #  ---> JSON\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Creates a session on a local master\n",
    "    spark = SparkSession.builder.appName(\"Restaurants in Wake County, NC\") \\\n",
    "        .master(\"local[*]\").getOrCreate()\n",
    "\n",
    "    # Comment this line to see full log\n",
    "    spark.sparkContext.setLogLevel('warn')\n",
    "    main(spark)\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01454cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/17 09:40:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+--------------------+--------+-----------+-----+----------+--------------+-------------------+-----------------+--------+------------+-----------+-------------+\n",
      "|OBJECTID|    HSISID|                NAME|            ADDRESS1|ADDRESS2|       CITY|STATE|POSTALCODE|   PHONENUMBER| RESTAURANTOPENDATE|     FACILITYTYPE|PERMITID|           X|          Y|GEOCODESTATUS|\n",
      "+--------+----------+--------------------+--------------------+--------+-----------+-----+----------+--------------+-------------------+-----------------+--------+------------+-----------+-------------+\n",
      "|    1001|4092016024|                WABA|2502 1/2 HILLSBOR...|    NULL|    RALEIGH|   NC|     27607|(919) 833-1710|2011-10-18 04:00:00|       Restaurant|    6952|-78.66818477|35.78783803|            M|\n",
      "|    1002|4092021693|  WALMART DELI #2247|2010 KILDAIRE FAR...|    NULL|       CARY|   NC|     27518|(919) 852-6651|2011-11-08 04:00:00|       Food Stand|    6953|-78.78211173|35.73717591|            M|\n",
      "|    1003|4092017012|CAROLINA SUSHI &a...|5951-107 POYNER V...|    NULL|    RALEIGH|   NC|     27616|(919) 981-5835|2015-08-28 03:00:00|       Restaurant|    6961|-78.57030208|35.86511564|            M|\n",
      "|    1004|4092030288|THE CORNER VENEZU...|    7500 RAMBLE WAY |    NULL|    RALEIGH|   NC|     27616|          NULL|2015-09-04 03:00:00|Mobile Food Units|    6962|  -78.537511|35.87630712|            M|\n",
      "|    1005|4092015530|        SUBWAY #3726| 12233 CAPITAL BLVD |    NULL|WAKE FOREST|   NC|27587-6200|(919) 556-8266|2009-12-11 03:00:00|       Restaurant|    6972|-78.54097555|35.98087357|            M|\n",
      "+--------+----------+--------------------+--------------------+--------+-----------+-----+----------+--------------+-------------------+-----------------+--------+------------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- OBJECTID: integer (nullable = true)\n",
      " |-- HSISID: long (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- ADDRESS1: string (nullable = true)\n",
      " |-- ADDRESS2: string (nullable = true)\n",
      " |-- CITY: string (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- POSTALCODE: string (nullable = true)\n",
      " |-- PHONENUMBER: string (nullable = true)\n",
      " |-- RESTAURANTOPENDATE: timestamp (nullable = true)\n",
      " |-- FACILITYTYPE: string (nullable = true)\n",
      " |-- PERMITID: integer (nullable = true)\n",
      " |-- X: double (nullable = true)\n",
      " |-- Y: double (nullable = true)\n",
      " |-- GEOCODESTATUS: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:*** Schema as a tree:\n",
      "WARNING:root:*** Schema as string: StructType([StructField('datasetId', LongType(), True), StructField('name', StringType(), True), StructField('address1', StringType(), True), StructField('address2', StringType(), True), StructField('city', StringType(), True), StructField('state', StringType(), True), StructField('zip', StringType(), True), StructField('tel', StringType(), True), StructField('dateStart', TimestampType(), True), StructField('type', StringType(), True), StructField('geoX', DoubleType(), True), StructField('geoY', DoubleType(), True), StructField('county', StringType(), False), StructField('id', StringType(), True)])\n",
      "WARNING:root:*** Schema as JSON: {\n",
      "  \"fields\": [\n",
      "    {\n",
      "      \"metadata\": {},\n",
      "      \"name\": \"datasetId\",\n",
      "      \"nullable\": true,\n",
      "      \"type\": \"long\"\n",
      "    },\n",
      "    {\n",
      "      \"metadata\": {},\n",
      "      \"name\": \"name\",\n",
      "      \"nullable\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    {\n",
      "      \"metadata\": {},\n",
      "      \"name\": \"address1\",\n",
      "      \"nullable\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    {\n",
      "      \"metadata\": {},\n",
      "      \"name\": \"address2\",\n",
      "      \"nullable\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    {\n",
      "      \"metadata\": {},\n",
      "      \"name\": \"city\",\n",
      "      \"nullable\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    {\n",
      "      \"metadata\": {},\n",
      "      \"name\": \"state\",\n",
      "      \"nullable\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    {\n",
      "      \"metadata\": {},\n",
      "      \"name\": \"zip\",\n",
      "      \"nullable\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    {\n",
      "      \"metadata\": {},\n",
      "      \"name\": \"tel\",\n",
      "      \"nullable\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    {\n",
      "      \"metadata\": {},\n",
      "      \"name\": \"dateStart\",\n",
      "      \"nullable\": true,\n",
      "      \"type\": \"timestamp\"\n",
      "    },\n",
      "    {\n",
      "      \"metadata\": {},\n",
      "      \"name\": \"type\",\n",
      "      \"nullable\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    {\n",
      "      \"metadata\": {},\n",
      "      \"name\": \"geoX\",\n",
      "      \"nullable\": true,\n",
      "      \"type\": \"double\"\n",
      "    },\n",
      "    {\n",
      "      \"metadata\": {},\n",
      "      \"name\": \"geoY\",\n",
      "      \"nullable\": true,\n",
      "      \"type\": \"double\"\n",
      "    },\n",
      "    {\n",
      "      \"metadata\": {},\n",
      "      \"name\": \"county\",\n",
      "      \"nullable\": false,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    {\n",
      "      \"metadata\": {},\n",
      "      \"name\": \"id\",\n",
      "      \"nullable\": true,\n",
      "      \"type\": \"string\"\n",
      "    }\n",
      "  ],\n",
      "  \"type\": \"struct\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datasetId: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- address1: string (nullable = true)\n",
      " |-- address2: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip: string (nullable = true)\n",
      " |-- tel: string (nullable = true)\n",
      " |-- dateStart: timestamp (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- geoX: double (nullable = true)\n",
      " |-- geoY: double (nullable = true)\n",
      " |-- county: string (nullable = false)\n",
      " |-- id: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "   Introspection of a schema.ch03\n",
    "\n",
    "   @author rambabu.posa\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import sys as sys\n",
    "\n",
    "def get_absolute_file_path(path, filename):\n",
    "        # To get absolute path for a given filename\n",
    "        current_dir = os.path.dirname(\"\")\n",
    "        relative_path = \"{}{}\".format(path, filename)\n",
    "        absolute_file_path = os.path.join(current_dir, relative_path)\n",
    "        return absolute_file_path\n",
    "\n",
    "def main(spark):\n",
    "        # The processing code.\n",
    "        filename = 'Restaurants_in_Wake_County_NC.csv'\n",
    "        #path = '../../../../data/'\n",
    "        path = '/home/vk/Spark_Source_Code/net.jgp.books.spark.ch03/data/'\n",
    "        absolute_file_path = get_absolute_file_path(path, filename)\n",
    "        # Reads a CSV file with header, called\n",
    "        # Restaurants_in_Wake_County_NC.csv,\n",
    "        # stores it in a dataframe\n",
    "        df = spark.read.csv(header=True, inferSchema=True, path=absolute_file_path)\n",
    "        \n",
    "        df.show(5)\n",
    "        df.printSchema()\n",
    "\n",
    "        # Let's transform our dataframe\n",
    "        df = df.withColumn(\"county\", F.lit(\"Wake\")) \\\n",
    "                .withColumnRenamed(\"HSISID\", \"datasetId\") \\\n",
    "                .withColumnRenamed(\"NAME\", \"name\") \\\n",
    "                .withColumnRenamed(\"ADDRESS1\", \"address1\") \\\n",
    "                .withColumnRenamed(\"ADDRESS2\", \"address2\") \\\n",
    "                .withColumnRenamed(\"CITY\", \"city\") \\\n",
    "                .withColumnRenamed(\"STATE\", \"state\") \\\n",
    "                .withColumnRenamed(\"POSTALCODE\", \"zip\") \\\n",
    "                .withColumnRenamed(\"PHONENUMBER\", \"tel\") \\\n",
    "                .withColumnRenamed(\"RESTAURANTOPENDATE\", \"dateStart\") \\\n",
    "                .withColumnRenamed(\"FACILITYTYPE\", \"type\") \\\n",
    "                .withColumnRenamed(\"X\", \"geoX\") \\\n",
    "                .withColumnRenamed(\"Y\", \"geoY\") \\\n",
    "                .drop(\"OBJECTID\", \"PERMITID\", \"GEOCODESTATUS\")\n",
    "\n",
    "        df = df.withColumn(\"id\",\n",
    "                F.concat(F.col(\"state\"), F.lit(\"_\"), F.col(\"county\"), F.lit(\"_\"), F.col(\"datasetId\")))\n",
    "\n",
    "        logging.warning(\"*** Schema as a tree:\")\n",
    "        print(df.printSchema())\n",
    "\n",
    "        logging.warning(\"*** Schema as string: {}\".format(df.schema))\n",
    "        schemaAsJson = df.schema.json()\n",
    "        parsedSchemaAsJson = json.loads(schemaAsJson)\n",
    "\n",
    "        logging.warning(\"*** Schema as JSON: {}\".format(json.dumps(parsedSchemaAsJson, indent=2)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "        # Creates a session on a local master\n",
    "        spark = SparkSession.builder \\\n",
    "                .appName(\"Schema introspection for restaurants in Wake County, NC\") \\\n",
    "                .master(\"local[*]\").getOrCreate()\n",
    "\n",
    "        # Comment this line to see full log\n",
    "        spark.sparkContext.setLogLevel('warn')\n",
    "        main(spark)\n",
    "        # Good to stop SparkSession at the end of the application\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fb4fefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:*** Right after ingestion\n",
      "WARNING:root:We have <bound method DataFrame.count of DataFrame[datasetid: string, fields: struct<closing_date:string,est_group_desc:string,geolocation:array<double>,hours_of_operation:string,id:string,insp_freq:bigint,opening_date:string,premise_address1:string,premise_address2:string,premise_city:string,premise_name:string,premise_phone:string,premise_state:string,premise_zip:string,risk:bigint,rpt_area_desc:string,seats:bigint,sewage:string,smoking_allowed:string,status:string,transitional_type_desc:string,type_description:string,water:string>, geometry: struct<coordinates:array<double>,type:string>, record_timestamp: string, recordid: string]> records.\n",
      "WARNING:root:*** Dataframe transformed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|       datasetid|              fields|            geometry|    record_timestamp|            recordid|\n",
      "+----------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|restaurants-data|{NULL, Full-Servi...|{[-78.9573299, 35...|2017-07-13T09:15:...|1644654b953d1802c...|\n",
      "|restaurants-data|{NULL, Nursing Ho...|{[-78.8895483, 36...|2017-07-13T09:15:...|93573dbf8c9e799d8...|\n",
      "|restaurants-data|{NULL, Fast Food ...|{[-78.9593263, 35...|2017-07-13T09:15:...|0d274200c7cef50d0...|\n",
      "|restaurants-data|{NULL, Full-Servi...|{[-78.9060312, 36...|2017-07-13T09:15:...|cf3e0b175a6ebad2a...|\n",
      "|restaurants-data|{NULL, NULL, [36....|{[-78.9135175, 36...|2017-07-13T09:15:...|e796570677f7c39cc...|\n",
      "+----------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- datasetid: string (nullable = true)\n",
      " |-- fields: struct (nullable = true)\n",
      " |    |-- closing_date: string (nullable = true)\n",
      " |    |-- est_group_desc: string (nullable = true)\n",
      " |    |-- geolocation: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- hours_of_operation: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- insp_freq: long (nullable = true)\n",
      " |    |-- opening_date: string (nullable = true)\n",
      " |    |-- premise_address1: string (nullable = true)\n",
      " |    |-- premise_address2: string (nullable = true)\n",
      " |    |-- premise_city: string (nullable = true)\n",
      " |    |-- premise_name: string (nullable = true)\n",
      " |    |-- premise_phone: string (nullable = true)\n",
      " |    |-- premise_state: string (nullable = true)\n",
      " |    |-- premise_zip: string (nullable = true)\n",
      " |    |-- risk: long (nullable = true)\n",
      " |    |-- rpt_area_desc: string (nullable = true)\n",
      " |    |-- seats: long (nullable = true)\n",
      " |    |-- sewage: string (nullable = true)\n",
      " |    |-- smoking_allowed: string (nullable = true)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- transitional_type_desc: string (nullable = true)\n",
      " |    |-- type_description: string (nullable = true)\n",
      " |    |-- water: string (nullable = true)\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- record_timestamp: string (nullable = true)\n",
      " |-- recordid: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:*** Looking at partitions\n",
      "/tmp/ipykernel_13767/1294692902.py:60: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  logging.warn(\"Partition count before repartition: {}\".format(partitionCount))\n",
      "WARNING:root:Partition count before repartition: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+------+---------+--------------------+--------------------+\n",
      "|             id|state|county|datasetId|                name|            address1|\n",
      "+---------------+-----+------+---------+--------------------+--------------------+\n",
      "|NC_Durham_56060|   NC|Durham|    56060|    WEST 94TH ST PUB| 4711 HOPE VALLEY RD|\n",
      "|NC_Durham_58123|   NC|Durham|    58123|BROOKDALE DURHAM IFS|4434 BEN FRANKLIN...|\n",
      "|NC_Durham_70266|   NC|Durham|    70266|       SMOOTHIE KING|1125 W. NC HWY 54...|\n",
      "|NC_Durham_97837|   NC|Durham|    97837|HAMPTON INN & SUITES|   1542 N GREGSON ST|\n",
      "|NC_Durham_60690|   NC|Durham|    60690|BETTER LIVING CON...|       909 GARCIA ST|\n",
      "+---------------+-----+------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- datasetId: string (nullable = true)\n",
      " |-- fields: struct (nullable = true)\n",
      " |    |-- closing_date: string (nullable = true)\n",
      " |    |-- est_group_desc: string (nullable = true)\n",
      " |    |-- geolocation: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- hours_of_operation: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- insp_freq: long (nullable = true)\n",
      " |    |-- opening_date: string (nullable = true)\n",
      " |    |-- premise_address1: string (nullable = true)\n",
      " |    |-- premise_address2: string (nullable = true)\n",
      " |    |-- premise_city: string (nullable = true)\n",
      " |    |-- premise_name: string (nullable = true)\n",
      " |    |-- premise_phone: string (nullable = true)\n",
      " |    |-- premise_state: string (nullable = true)\n",
      " |    |-- premise_zip: string (nullable = true)\n",
      " |    |-- risk: long (nullable = true)\n",
      " |    |-- rpt_area_desc: string (nullable = true)\n",
      " |    |-- seats: long (nullable = true)\n",
      " |    |-- sewage: string (nullable = true)\n",
      " |    |-- smoking_allowed: string (nullable = true)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- transitional_type_desc: string (nullable = true)\n",
      " |    |-- type_description: string (nullable = true)\n",
      " |    |-- water: string (nullable = true)\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- record_timestamp: string (nullable = true)\n",
      " |-- recordid: string (nullable = true)\n",
      " |-- county: string (nullable = false)\n",
      " |-- name: string (nullable = true)\n",
      " |-- address1: string (nullable = true)\n",
      " |-- address2: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip: string (nullable = true)\n",
      " |-- tel: string (nullable = true)\n",
      " |-- dateStart: string (nullable = true)\n",
      " |-- dateEnd: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- geoX: double (nullable = true)\n",
      " |-- geoY: double (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Partition count after repartition: 4\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "   CSV ingestion in a dataframe. ch220\n",
    "\n",
    "   @author rambabu.posa\n",
    "\"\"\"\n",
    "import os\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "#from pyspark.sql.functions import F\n",
    "import pyspark.sql.functions as F\n",
    "import json\n",
    "\n",
    "def get_absolute_file_path(path, filename):\n",
    "        # To get absolute path for a given filename\n",
    "        #current_dir = os.path.dirname(__file__)\n",
    "        current_dir = '/home/vk/Spark_Source_Code/net.jgp.books.spark.ch03/src/main/python/lab220_json_ingestion_schema_manipulation/'\n",
    "        relative_path = \"{}{}\".format(path, filename)\n",
    "        absolute_file_path = os.path.join(current_dir, relative_path)\n",
    "        return absolute_file_path\n",
    "\n",
    "def main(spark):\n",
    "        # The processing code.\n",
    "        filename = 'Restaurants_in_Durham_County_NC.json'\n",
    "        path = '../../../../data/'\n",
    "        absolute_file_path = get_absolute_file_path(path, filename)\n",
    "\n",
    "        # Reads a JSON file called Restaurants_in_Durham_County_NC.json, stores\n",
    "        # it in a dataframe\n",
    "        df = spark.read.json(absolute_file_path)\n",
    "        logging.warning(\"*** Right after ingestion\")\n",
    "        df.show(5)\n",
    "        df.printSchema()\n",
    "        logging.warning(\"We have {} records.\".format(df.count))\n",
    "\n",
    "        df =  df.withColumn(\"county\", F.lit(\"Durham\")) \\\n",
    "                .withColumn(\"datasetId\", F.col(\"fields.id\")) \\\n",
    "                .withColumn(\"name\", F.col(\"fields.premise_name\")) \\\n",
    "                .withColumn(\"address1\", F.col(\"fields.premise_address1\")) \\\n",
    "                .withColumn(\"address2\", F.col(\"fields.premise_address2\")) \\\n",
    "                .withColumn(\"city\", F.col(\"fields.premise_city\")) \\\n",
    "                .withColumn(\"state\", F.col(\"fields.premise_state\")) \\\n",
    "                .withColumn(\"zip\", F.col(\"fields.premise_zip\")) \\\n",
    "                .withColumn(\"tel\", F.col(\"fields.premise_phone\")) \\\n",
    "                .withColumn(\"dateStart\", F.col(\"fields.opening_date\")) \\\n",
    "                .withColumn(\"dateEnd\", F.col(\"fields.closing_date\")) \\\n",
    "                .withColumn(\"type\", F.split(F.col(\"fields.type_description\"), \" - \").getItem(1)) \\\n",
    "                .withColumn(\"geoX\", F.col(\"fields.geolocation\").getItem(0)) \\\n",
    "                .withColumn(\"geoY\", F.col(\"fields.geolocation\").getItem(1))\n",
    "\n",
    "        df = df.withColumn(\"id\", F.concat(F.col(\"state\"), F.lit(\"_\"),\n",
    "                                          F.col(\"county\"), F.lit(\"_\"),\n",
    "                                          F.col(\"datasetId\")))\n",
    "\n",
    "        logging.warning(\"*** Dataframe transformed\")\n",
    "        df.select('id',\"state\", \"county\", \"datasetId\", \"name\", \"address1\").show(5)\n",
    "        df.printSchema()\n",
    "\n",
    "        logging.warning(\"*** Looking at partitions\")\n",
    "        partitionCount = df.rdd.getNumPartitions()\n",
    "        logging.warn(\"Partition count before repartition: {}\".format(partitionCount))\n",
    "\n",
    "        df = df.repartition(4)\n",
    "        logging.warning(\"Partition count after repartition: {}\".format(df.rdd.getNumPartitions()))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "        # Creates a session on a local master\n",
    "        spark = SparkSession.builder.appName(\"Restaurants in Durham County, NC\") \\\n",
    "                .master(\"local[*]\").getOrCreate()\n",
    "\n",
    "        # Comment this line to see full log\n",
    "        spark.sparkContext.setLogLevel('warn')\n",
    "        main(spark)\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91a91bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------+-----------+-----+----------+--------------+-------------------+-----------------+------------+-----------+------+-------+------------------+\n",
      "| datasetId|                name|            address1|address2|       city|state|       zip|           tel|          dateStart|             type|        geoX|       geoY|county|dateEnd|                id|\n",
      "+----------+--------------------+--------------------+--------+-----------+-----+----------+--------------+-------------------+-----------------+------------+-----------+------+-------+------------------+\n",
      "|4092016024|                WABA|2502 1/2 HILLSBOR...|    NULL|    RALEIGH|   NC|     27607|(919) 833-1710|2011-10-18 04:00:00|       Restaurant|-78.66818477|35.78783803|  Wake|   NULL|NC_Wake_4092016024|\n",
      "|4092021693|  WALMART DELI #2247|2010 KILDAIRE FAR...|    NULL|       CARY|   NC|     27518|(919) 852-6651|2011-11-08 04:00:00|       Food Stand|-78.78211173|35.73717591|  Wake|   NULL|NC_Wake_4092021693|\n",
      "|4092017012|CAROLINA SUSHI &a...|5951-107 POYNER V...|    NULL|    RALEIGH|   NC|     27616|(919) 981-5835|2015-08-28 03:00:00|       Restaurant|-78.57030208|35.86511564|  Wake|   NULL|NC_Wake_4092017012|\n",
      "|4092030288|THE CORNER VENEZU...|    7500 RAMBLE WAY |    NULL|    RALEIGH|   NC|     27616|          NULL|2015-09-04 03:00:00|Mobile Food Units|  -78.537511|35.87630712|  Wake|   NULL|NC_Wake_4092030288|\n",
      "|4092015530|        SUBWAY #3726| 12233 CAPITAL BLVD |    NULL|WAKE FOREST|   NC|27587-6200|(919) 556-8266|2009-12-11 03:00:00|       Restaurant|-78.54097555|35.98087357|  Wake|   NULL|NC_Wake_4092015530|\n",
      "+----------+--------------------+--------------------+--------+-----------+-----+----------+--------------+-------------------+-----------------+------------+-----------+------+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- datasetId: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- address1: string (nullable = true)\n",
      " |-- address2: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip: string (nullable = true)\n",
      " |-- tel: string (nullable = true)\n",
      " |-- dateStart: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- geoX: double (nullable = true)\n",
      " |-- geoY: double (nullable = true)\n",
      " |-- county: string (nullable = false)\n",
      " |-- dateEnd: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:We have 5903 records.\n",
      "WARNING:root:Partition count: 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "   Union of two dataframes. ch 3, lab 230\n",
    "\n",
    "   @author rambabu.posa\n",
    "\"\"\"\n",
    "import os\n",
    "import util\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def get_absolute_file_path(path, filename):\n",
    "    # To get absolute path for a given filename\n",
    "    #current_dir = os.path.dirname(__file__)\n",
    "    current_dir = '/home/vk/Spark_Source_Code/net.jgp.books.spark.ch03/src/main/python/lab230_dataframe/'\n",
    "    relative_path = \"{}{}\".format(path, filename)\n",
    "    absolute_file_path = os.path.join(current_dir, relative_path)\n",
    "    return absolute_file_path\n",
    "\n",
    "def main(spark):\n",
    "    # The processing code.\n",
    "    filename1 = 'Restaurants_in_Wake_County_NC.csv'\n",
    "    path1 = '../../../../data/'\n",
    "    absolute_file_path1 = get_absolute_file_path(path1, filename1)\n",
    "\n",
    "    filename2 = 'Restaurants_in_Durham_County_NC.json'\n",
    "    path2 = '../../../../data/'\n",
    "    absolute_file_path2 = get_absolute_file_path(path2, filename2)\n",
    "\n",
    "    df1 = spark.read.csv(path=absolute_file_path1, header=True, inferSchema=True)\n",
    "\n",
    "    df2 = spark.read.json(absolute_file_path2)\n",
    "\n",
    "    wakeRestaurantsDf = util.build_wake_restaurants_dataframe(df1)\n",
    "    durhamRestaurantsDf = util.build_durham_restaurants_dataframe(df2)\n",
    "\n",
    "    util.combineDataframes(wakeRestaurantsDf, durhamRestaurantsDf)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Creates a session on a local master\n",
    "    spark = SparkSession.builder.appName(\"Union of two dataframes\") \\\n",
    "        .master(\"local[*]\").getOrCreate()\n",
    "\n",
    "    # Comment this line to see full log\n",
    "    spark.sparkContext.setLogLevel('warn')\n",
    "    main(spark)\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e13e85d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/18 14:44:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|  Jean|\n",
      "|   Liz|\n",
      "|Pierre|\n",
      "|Lauric|\n",
      "+------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "  Converts an array to a Dataframe of strings.\n",
    "\n",
    "   @author rambabu.posa\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType\n",
    "\n",
    "\n",
    "def main(spark):\n",
    "    data = [['Jean'], ['Liz'], ['Pierre'], ['Lauric']]\n",
    "\n",
    "    \"\"\"\n",
    "    * data:    parameter list1, data to create a dataset\n",
    "    * encoder: parameter list2, implicit encoder\n",
    "    \"\"\"\n",
    "    schema = StructType([StructField('name', StringType(), True)])\n",
    "\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    df.show()\n",
    "    df.printSchema()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Creates a session on a local master\n",
    "    spark = SparkSession.builder.appName(\"Array to Dataframe\") \\\n",
    "        .master(\"local[*]\").getOrCreate()\n",
    "\n",
    "    # Comment this line to see full log\n",
    "    spark.sparkContext.setLogLevel('warn')\n",
    "    main(spark)\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa07ec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Spark to Pandas DataFrame. Here 22 row in file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+-----------+--------------------+\n",
      "| id|authorId|               title|releaseDate|                link|\n",
      "+---+--------+--------------------+-----------+--------------------+\n",
      "|  1|       1|Fantastic Beasts ...|   11/18/16|http://amzn.to/2k...|\n",
      "|  2|       1|Harry Potter and ...|    10/6/15|http://amzn.to/2l...|\n",
      "|  3|       1|The Tales of Beed...|    12/4/08|http://amzn.to/2k...|\n",
      "|  4|       1|Harry Potter and ...|    10/4/16|http://amzn.to/2k...|\n",
      "|  5|       2|Informix 12.10 on...|    4/23/17|http://amzn.to/2i...|\n",
      "|  6|       2|Development Tools...|   12/28/16|http://amzn.to/2v...|\n",
      "|  7|       3|Adventures of Huc...|    5/26/94|http://amzn.to/2w...|\n",
      "|  8|       3|A Connecticut Yan...|    6/17/17|http://amzn.to/2x...|\n",
      "| 10|       4|Jacques le Fataliste|     3/1/00|http://amzn.to/2u...|\n",
      "| 11|       4|Diderot Encyclope...|       NULL|http://amzn.to/2i...|\n",
      "| 12|    NULL|   A Woman in Berlin|    7/11/06|http://amzn.to/2i...|\n",
      "| 13|       6|Spring Boot in Ac...|     1/3/16|http://amzn.to/2h...|\n",
      "| 14|       6|Spring in Action:...|   11/28/14|http://amzn.to/2y...|\n",
      "| 15|       7|Soft Skills: The ...|   12/29/14|http://amzn.to/2z...|\n",
      "| 16|       8|     Of Mice and Men|       NULL|http://amzn.to/2z...|\n",
      "| 17|       9|Java 8 in Action:...|    8/28/14|http://amzn.to/2i...|\n",
      "| 18|      12|              Hamlet|     6/8/12|http://amzn.to/2y...|\n",
      "| 19|      13|             Pensées| 12/31/1670|http://amzn.to/2j...|\n",
      "| 20|      14|Fables choisies, ...|   9/1/1999|http://amzn.to/2y...|\n",
      "| 21|      15|Discourse on Meth...|  6/15/1999|http://amzn.to/2h...|\n",
      "+---+--------+--------------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- authorId: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- releaseDate: string (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      "\n",
      "   id  authorId                                              title releaseDate\n",
      "0   1       1.0  Fantastic Beasts and Where to Find Them: The O...    11/18/16\n",
      "1   2       1.0  Harry Potter and the Sorcerer's Stone: The Ill...     10/6/15\n",
      "2   3       1.0  The Tales of Beedle the Bard, Standard Edition...     12/4/08\n",
      "3   4       1.0  Harry Potter and the Chamber of Secrets: The I...     10/4/16\n",
      "4   5       2.0  Informix 12.10 on Mac 10.12 with a dash of Jav...     4/23/17\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Back from Pandas to Spark DataFrame. Here 5 rows now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+-----------+\n",
      "| id|authorId|               title|releaseDate|\n",
      "+---+--------+--------------------+-----------+\n",
      "|  1|     1.0|Fantastic Beasts ...|   11/18/16|\n",
      "|  2|     1.0|Harry Potter and ...|    10/6/15|\n",
      "|  3|     1.0|The Tales of Beed...|    12/4/08|\n",
      "|  4|     1.0|Harry Potter and ...|    10/4/16|\n",
      "|  5|     2.0|Informix 12.10 on...|    4/23/17|\n",
      "+---+--------+--------------------+-----------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- authorId: double (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- releaseDate: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ch 3 lab 320 author: Vadim Kisarov\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "import pandas\n",
    "\n",
    "# main() must be before her invoke!\n",
    "def main(spark):\n",
    "    path = \"/home/vk/Spark_Source_Code/net.jgp.books.spark.ch03/data/books.csv\"\n",
    "    df = spark.read.csv(header = True,  inferSchema = True, path = path)\n",
    "    \n",
    "    logging.warning(\"Spark to Pandas DataFrame. Here {} row in file\".format(df.count()))\n",
    "    df.show()\n",
    "    df.printSchema()\n",
    "    \n",
    "    df_pandas = df.toPandas()\n",
    "    df_pandas = df_pandas[:5][['id', 'authorId', 'title', 'releaseDate']]\n",
    "    print(df_pandas)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    releaseDate = df_pandas[\"releaseDate\"]\n",
    "    \n",
    "    df_spark = spark.createDataFrame(df_pandas)\n",
    "    logging.warning(\"Back from Pandas to Spark DataFrame. Here {} rows now\".format(df_spark.count()))\n",
    "    df_spark.show()\n",
    "    df_spark.printSchema()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    spark = SparkSession.builder.appName(\"CSV to dataframe to dataset and back\") \\\n",
    "        .master(\"local\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    spark.sparkContext.setLogLevel('warn')\n",
    "    main(spark)\n",
    "    spark.stop()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1916cb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-11-18\n",
      "2015-10-06\n",
      "2017-04-23\n",
      "1670-12-31\n",
      "1999-06-15\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "releaseDate = pd.Series(['11/18/16', '10/6/15', '4/23/17', '12/31/1670', '6/15/1999', 'NULL'])\n",
    "releaseDate\n",
    "date_list = []\n",
    "for string in releaseDate:\n",
    "    spl_arrow = string.split(\"/\")\n",
    "    if len(spl_arrow) > 1:\n",
    "       year = '20' + spl_arrow[2] if len(spl_arrow[2]) == 2 else spl_arrow[2]\n",
    "       month = '0' + spl_arrow[0] if len(spl_arrow[0]) == 1 else spl_arrow[0]\n",
    "       day = '0' + spl_arrow[1] if len(spl_arrow[1]) == 1 else spl_arrow[1]\n",
    "       print(f\"{year}-{month}-{day}\")\n",
    "      \n",
    "    \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859f4c07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
